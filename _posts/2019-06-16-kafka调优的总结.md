---
layout:     post
title:      kafka性能调优的总结
subtitle:   kafka
date:       2019-6-16
author:     BY
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - kafka
    - 消息队列
    - 中间件
---

## Kafka的调优的一点总结

关于kafka的一些基础知识我想大家都知道，今天抽空整理下这些年使用kafka的一些调优技巧



而对于kafka的性能我们有两个指标，一个是延迟率，一个是吞吐量。所谓延迟率，就是处理一个事件需要多少事件。吞吐率则是说特定的时间内有多少事件到达。他们之间其实是相互平衡的。如果kafka有足够的broker去处理Topic，那么他一定是需要一定的延迟去处理这些消息的。



我们从Broker，Producer，Consumer这3个主要方面来分析

> 因为我们主要作为一个开发那么我们从应用的角度来看，所以这里不从服务器配置以及os系统方面来讨论，但这块对kafka的影响也有一定的影响，日后有调试到这一块的时候，再做补充，留个ToDo

### broker

我们知道broker表示每一个kafka实例，一个集群里面一般会有多个broker组成，每个broker里面会有多个Topic，而我们依靠zookeeper在做调度的时候，就是多个broker在zookeeper上注册。

有了topic相应的就会涉及到partition,这样我们可知影响broker的重要点就是topic，partition


#### Topic

1. 设置带key的topic

	为什么要设置带key的topic，因为在kafka的producer的代码里有这样一段
	
	```java
	
	public int partition(Object key, int numPartitions) {
		int partition = 0;
		String k = (String)key;
		partition = Math.abs(k.hashCode()) % numPartitions;
		return partition;
	}

	```
	
	可以看到kafka的producer将消息的key做了hashcode，然后和分区数（numPartitions）做模运算，使得每一个key都可以分布到一个分区中

	保证了同一个topic的数据，尽量落在同一个分区里，这样降低了数据丢失的概率，也降低了数据再做batch的时候，所造成的性能压力
	
	***但是，这样会降低一定的吞吐量，因为只有一个分区了，所以按需配置***
	
2. 创建带分区的topic

	创建带分区的topic的意思就是在多个broker会存在多个topic的副本，这样做是为了更好的做负载均衡，这样在kafka的replica算法下，每个集群下Partition会取模然后分配到特定的broker上。
	
	```java
	
	  if (opts.options.has(opts.replicaAssignmentOpt)) {//检测是否有replica-assignment参数
      val assignment = parseReplicaAssignment(opts.options.valueOf(opts.replicaAssignmentOpt))
      AdminUtils.createOrUpdateTopicPartitionAssignmentPathInZK(zkUtils, topic, assignment, configs, update = false)
    } else {
      CommandLineUtils.checkRequiredArgs(opts.parser, opts.options, opts.partitionsOpt, opts.replicationFactorOpt)
      val partitions = opts.options.valueOf(opts.partitionsOpt).intValue
      val replicas = opts.options.valueOf(opts.replicationFactorOpt).intValue
      val rackAwareMode = if (opts.options.has(opts.disableRackAware)) RackAwareMode.Disabled
                          else RackAwareMode.Enforced
      AdminUtils.createTopic(zkUtils, topic, partitions, replicas, configs, rackAwareMode)
    }

	
	```
	
	截取一段kafka在创建topic的时候，如果加入了了**zkUtils**的zk配置，也就是创建分区，那么的话，他会按照分区算出他的模，也就是rackAwareMode值
	
	
#### partition

partition是kafka存储的物理单位，一个partition里面就是存储我们发送消费的消息，他们都是顺序存储的，通过offset来标识。

partition中有个重要的标识**segment**，segment对应一个文件，而partition就是一个文件夹，一个partition里面可以有多个segment，至于为什么要引入segment，我们联想下，如果没有文件，那么这个文件夹就会一直增大，每次在做数据清洗的时候，都需要把前面的所有数据清除，想想都是个大工程，所以segment的引入，我们每次清洗的时候，只需要把segment清洗掉就行了。


1.按照吞吐量来设置partition

   - 增加吞吐量：对于 consumer 来说，一个 partition 只能被一个 consumer 线程所消费，适当增加 partition 数，可以增加 consumer 的并发，进而增加系统的吞吐量

***但是！按照我们上面所说，partition数量的增加会带来更多的segment。对于每一个 segment，在 broker 都会有一个对应的 index 和实际数据文件，而对于 Kafka Broker，它将会对于每个 segment 每个 index 和数据文件都会打开相应的 file handle***
	
***另一个问题broker一旦发生download，那么会重新选举，这时候，大量的partition会带来一定的不可用时间，这就很麻烦了，会出现重复消费，或者消息丢失的情况***
	
2. 限制partition的数量

- 这个呢，能保证到一定的顺序性，以及减少因为partition数量多，而造成的性能影响，但是，他会降低吞吐量，consumer少了毕竟

***所以，对于partition数量的控制，如果我们业务需要足够大的吞吐，并且容忍一定的丢失（这个取决于auto.offset的设置），那么我们多个partiton完全可用，而且也加大了系统的可用性。如果我们不需要那么大的吞吐量，只需要保证顺序性，或者安全性，那么单个partition足够***


### producer

其实对于生产者来说，我们更关注的是消息的可达，以及kafka的压力我们适时的调整我们生产的速率。这边主要说两个参数

1. batch.size

	batch.size 也就是批量发送。这的批量发送，并不是批量的消息数，而是批量的字节数。他意味着他在向kafka发送消息之前会控制字节数，默认的是16348 byte (16k)，在不超过可用内存下，我们是可以尽可能的设置高些。



	当然了batch.size也不是越高越好的，如果我们的producer一直在发送消息，那么这时候吞吐量是能达到一个比较完美的情况，而如果producer经常闲置，也就是没有足够的数据来满足当前的内存分配，那么batch.siza将会白白占有很大的内存



2. Linger.ms


	延迟等待时间。比如说我们100ms内设置100个batch的批量发送，只有达到100个的时候，我们才会发送，当然了他一定是要同一个partition。这边增加了消息的延迟，但是提高了吞吐量。有点像TCP的多路复用，但不是同一个，我们都是将多个请求，合并成一个


	默认我们的是0ms，这也是为了保证消息的实时性。
	
	
	***总结下：避免大的消息，降低内存的使用量，提升broker的处理速度，频繁的网络IO会带来一定的压力对kafka***
	
	
### Consumer

	
所谓的消费者优化，其实说白了就是下游系统的消费能力。我们知道一个partition对应一个consumer，那么是不是consumer越多，消费能力就越强呢？当然不是了！

一个Topic只存在于一个broker，如果broker无法做到水平扩展，那么再多的消费者，也只能等待，造成下游系统的压力增加，那么我们怎么在kafka的consumer做优化呢？



1. max.poll.interval.ms


	我们在poll的时候，平均每个消息的处理时间。最合理的配置是两次消费之间的时间，因为如果设置过小，会导致group重新balance，而设置过大，则会导致group Rebalance



2. atuo.offset.reset

	atuo,offset,reset 有两个参数 latest,erliest,一个表示从最近的记录开始拉取，一个表示从起始位置读取partition，如果不想要丢失那么是erliest，如果容忍一定程度的丢失那么就latest。他只适用于偏移量无效的情况下，那么他什么时候会无效，broker重新rebalance的时候。相应的数据量的不同，下游的处理效率，kafka的吞吐量也不一样
	
***总结下：对于我们consumer来说，主要是消费能，那么的话我们要注意每次最大拉取的数量。只要提交offset的时间足够ok，那么减小max没问题，如果不ok，那么的话，时间我们用默认的就好了。而auto.offset要注意了，这是取决于你消息的丢失与否！***


### 总结

在应用层面，我们注意到这几个重要的优化调整指标其实就够了，不能盲目的加partition,也不能盲目的死命往里面塞数据，合理的规划，既保证了kafka的性能，也让业务方更加放心

>题外话: kafka的性能足够优异，apache官方给出的是可满足亿万级别的数据读写操作。我们压测的结果，在64G内存，500G硬盘，2.8Hz双核CPU，3个broker的情况下 对分区内的同一个topic进行写入，可满足每秒800W次的写入!

